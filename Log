Este Log serve como um acompanhamento das ideias que tive
ao longo do desenvolvimento da IA, tanto para poder relembrar
o que estava pensando caso pause por muito tempo, quanto para
poder ver como evoluí ao longo do processo. O vocabulário
utilizado não será formal nem informal demais. Com exceção
de algumas escolhas de palavras (para não ficar muito feio
em certos momentos), escrevi o que pensei na hora de fazer
estas anotações

Início~15/Maio/2018
  Coordenadas Sample.txt é um arquivo com o formato
    matricial de uma tela do jogo. printSample.py,
    da mesma pasta, imprimi coordenadasSample.txt
    em formato de texto, identificando pontuação
    e asteroides rosa e azul claro

  Descoberta: a tela impressa é baseada em uma matriz
    com as cores e coordenads de cada objeto presente
    na tela impressa. Ou seja, não existem matrizes
    (variáveis do tipo observation ou obs pelos nomes
    dados no programa que roda o jogo) que tenham
    tanto a nave e os tiros quanto os asteroides,
    apesar de suas respectivas posições serem guardadas
    na memória para o jogo progredir

  Ideias para features do approximate Q-learning:
    -Dentro de um certo raio em volta da nave, se houver
     algum asteroide, calcular a sua direção de movimento
     e reagir de acordo. Seria necessário criar um "círculo"
     de detecção em volta da nave em que apenas dentro dele seria
     verificado (pelo menos por um tempo - não sei se os
     asteroides vão ficando mais rápidos). Caso algum
     asteroide fosse detectado, ver onde está em um quadro,
     ver onde ele está na próxima vez que aparecer (maldito
     flickering do emulador) e, caso esteja vindo na direção
     da nave, sair do caminho ou atirar (a decidir)

  Stella (emulador de Atari):
    -Emulador Stella de Atari 2600 está funcionando, mas precisa
     apertar F2 no começo de toda partida para começar de verdade

================================

17/Maio/2018
  info: no caso do Asteroids.v0, é a quantidade de vida que a
    nave tem

  reward: recompensa recebida pela ação. Mesmo que uma ação não
    seja tomada, o valor é retornado quando um asteroide é
    destruído

  Curiosamente, quando a nave toca em um asteroide, o asteroide
    é destruído e a nave recebe a recompensa por isso. Porém,
    ela também perde, e o info recebido nessa atualização é
    da quantidade de vida que a nave tinha no momento que
    encostou no asteroide e não depois. A atualização da
    quantidade de vidas restantes ocorre alguns passos
    depois apenas

  Tentar usar o Gym Retro para ver se é mais fácil
    link: https://github.com/openai/retro

  ROM do Asteroids transferida para uma pasta 'ROMs' da
    pasta gym-retro

  Arquivo da pasta ~/gym-retro/examples/ alterado para
    renderizar a tela todos os frames, de forma que é possível
    visualizar a nave e os asteroides (continuam aparecendo em
    frames intercalados), bem como colocado uma pausa entre
    frames para ser possível visualizar e entender o que está
    acontecendo

================================

28/Maio/2018
  Adicionado um .sh que roda o random_agent.py mais facilmente
    (e para me ajudar a não esquecer como faz, já que eu gastei
    alguns minutos tentando lembrar)

  A variável info recebida a cada step é do tipo dict e armazena
    dois valores: as vidas (que, por algum motivo, é 16 vezes o
    número que aparece na tela - ou seja, 4 vidas na tela = 64
    vidas em info) e a pontuação total, só que dividida por 10.
    A tela provavelmente mostra a pontuação multiplicada por 10
    para dar a sensação de que tem muitos pontos.

================================

07/Jun/2018
  Considerando que a tela é uma matriz de 210, 160, 3, sendo 210
    o número de pixas da altura, 160 o número de pixels da
    largura e 3 é o número de cores (RGB) que cada pixel assume,
    sendo que cada um desses 3 números estão no intervalo
    [0, 255], o espaço efetivo que os asteroides e a nave estão
    (ou seja, desconsiderando bordas, espaço onde fica a
    pontuação e vida), é nos intervalos (indo de 0 a 219, 0 a 
    159 e 0 a 2):
    Altura: [18, 194]
    Largura: [8, 159]
    Em outras palavras, não preciso me preocupar com as
    primeiras 18 linhas (0 a 17), últimas 15 linhas
    (195 a 209) e as primeiras 8 colunas (0 a 7), pois
    é garantido que não haverá pixels dos asteroides
    nem da nave nesses espaços.

================================

09/Jun/2018
  Função que realiza uma BFS para encontrar os tamanhos e posições
    iniciais dos asteroides (considerando-os como retângulos e
    retornando os limites superior, inferior, esquerdo e direito
    desse retângulo) funcionando.

  A função env.render() só altera o que é impresso na tela quando
    env.reset() ou env.step(action) é chamado. Se o env.render()
    for chamado várias vezes seguidas sem nenhum desses métodos
    for chamado, a tela não muda

================================

22/Jun/2018
  Se o tempo for inicializado como t = 0, os asteroides são
    renderizados quando t%2==0 (t=0, t=2, t=4, ...) enquanto
    a nave e os tiros são renderizados quando t%2==1 (t=1, t=3,
    t=5, ...)

================================

07~09/Jul/2018
  Não sei se já anotei isso mais acima, mas há a possibilidade de haver
    diversos asteroids com a mesma cor, o que torna armazená-los pela cor
    ser inviável. Felizmente, no dicionário que armazena a cor e posição de
    cada asteroide, cada asteroide já é identificado por um número ID tal
    que ID = 0, 1, 2, ...

  Depois de deixar o jogo rodando com apenas o findInitialAsteroids(obs)
    sendo chamado, na segunda fase, percebi que a velocidade dos asteroids
    é a mesma que na fase 1. Por hora, vou concluir que a velocidade dos
    asteroids é sempre a mesma

  Preciso testar se o update_pos() está funcionando

  update_pos() foi alterado de forma que sempre consegue pegar os novos
    limiter superior, inferior, esquerdo e direito, mesmo que eles deem
    volta na tela. Por outro lado, o for que busca para onde o asteroide
    foi não funciona, então preciso pensar em outro método para fazer a
    busca. Está tarde por hoje, então deixo para fazer isso amanhã

================================

10/Jul/2018
  update_pos() está quase funcionando. Se rodar o myAi.py por um tempo
    e comprara a saída dos findInitialPos() com o update_pos(), há uma
    diferença quando os asteróides dão a volta na tela. update_pos()
    parece estar errado e tenho que descobrir o que é.

================================

11/Jul/2018
  update_pos() o problema é que, quando dá uma volta na tela, o diff
    aumenta ou diminui muito (diferença de antes para depois),
    porque se subtrai o número da linha de cada extremo da tela.
    Pensando como solucionar esse problema.
    Até o final do dia, não tinha conseguido solucionar ainda

================================

12/Jul/2018
  update_pos() acabei de descobrir o problema: por algum motivo,
    existe uma linha de "limbo" em que uma linha do asteróide
    pode ficar antes de dar a volta na tela (pelo menos por
    cima/baixo; a velocidade horizontal dos asteróides é
    muito baixa para eu me importar por hora). Ou seja, apesar
    de o asteróide se mover 2 linhas a cada aparição (ou seja,
    uma linha por frame, já que eles aparecem em frames
    intercalados), se a linha mais de cima do asteróide
    estiver na linha 18, ele não aparecerá na linha 193 em
    sua próxima aparição, mas sim na 194. Isso aconteceu
    nas poucas vezes que verifiquei e ele se movia para cima.
    O comportamento parece se manter quando o asteróide se
    move para baixo, mas, nesse caso, ele fica ainda mais tempo
    sem aparecer. Quando finalmente reaparece na parte de
    cima da tela, aparece um bloco inteiro do asteróide
    de uma vez, ficando alguns frames com um pedaço faltando
    até aparecer.

================================

14/Jul/2018
  update_pos()
    Definição dos limites de busca pela nova posição do
    asteróide movidas para a função get_search_bounds();
    
    Quando um asteróide é destruído, isso será detectado
    na primeira frame em que o asteróide aparece após
    ser destruído.

=================

15/Jul/2018
  update_pos()
    Em um teste que fiz hoje, descobri que alguns asteróides
    desaparecem quando um é destruído.
    A situação em que ocorreu foi: quando um asteróide pequeno
    foi destruído (chamaremos os iniciais de grande, intermediários
    de médios, e finais de pequenos), o outro asteróide pequeno
    e um grande sumiram da tela no frame em que ocorreu a
    destruição. Isso causou comportamente incorreto da IA: como
    ela verifica frame a frame quais asteróides estão na tela,
    esse sumiço fez ela assumir que o asteróide foi destruído.
    
    Vou tentar resolver o problema acima fazendo a busca
    pelo asteróide supostamente destruído uma segunda vez 
    em sua área para ver se realmente foi destruído.
    A atualização de espaço do asteróide será feita
    independente de ele ter sido destruído ou não, pois, se não
    tiver sido, a IA ainda saberá onde ele está.
    O corner case de haver mais um asteróide da mesma cor mais
    ou menos na mesma região persiste, mas espero que isso
    seja algo que a IA aprenda a lidar, da mesma forma
    que seres humanos
    
    Acho que ainda terei que olhar o código de novo para ter
    certeza de que não deixei passar nada. Uma ideia foi
    aumentar o intervalo de verificação (ao invés de a cada 2
    frames, que é quando os asteróides aparecem, fazer a cada
    4 frames), mas, mesmo não acontecendo o problema, me toquei
    que ele poderia simplesmente acontecer nos novos instantes
    em que a busca pela nova posição do asteróide é feita.
    Portanto, decidi simplesmente pular a primeira busca pela
    nova posição dos asteróides após alguma recompensa (positiva
    ou negativa) ser recebida (flag got_reward)
    
    Acho que ainda terei que olhar o código de novo para ter
    certeza de que não deixei passar nada. Uma ideia foi
    aumentar o intervalo de verificação (ao invés de a cada 2
    frames, que é quando os asteróides aparecem, fazer a cada
    4 frames), mas, mesmo não acontecendo o problema, me toquei
    que ele poderia simplesmente acontecer nos novos instantes
    em que a busca pela nova posição do asteróide é feita.
    
    Portanto, decidi simplesmente pular a primeira busca pela
    nova posição dos asteróides após alguma recompensa (positiva
    ou negativa) ser recebida (flag got_reward). Porém, ainda
    preciso atualizar a área de busca do asteróide para isso...

    Feito! Até que algum problema novo por causa do maldito emulador
    ocorra de novo, a busca pelos asteróides deve estar funcionando
    corretamente. Próximo passo é ele buscar pelos asteróides
    menores que surgem após a destruíção dos asteróides. Pode
    ser um pouco complicado já que eles podem aparecer da exata mesma
    cor que o asteróide que os originou

=================

16/Jul/2018
  main()
    quando uma pontuação é feita (um asteróide é destruído), a primeira
    busca pelas novas posições depois de marcar essa pontuação é pulada.
    Para compensar a busca é feita em uma área maior de tal forma
    que é como se a busca pela nova posição tivesse sido feita. Espera-se
    que isso resolva o problema de a IA achar que um asteróide foi
    destruído porque o emulador não o mostra quando um outro é destruído
    em alguns casos.

  A minha esperança é que nos casos em que a IA vir o mesmo asteróide
  múltiplas vezes pensando que são diferentes é que eles todos sejam
  destruídos e eliminados da visão da IA ao mesmo tempo, podendo talvez
  diminuir a eficiência, mas não quebrando completamente;

  posMethods():
    Acabei de lembrar que preciso arrumar o asteroidBFS() para poder
    buscar dando voltas na tela, para o caso de a tela inicial de uma
    fase conter asteróides dando a volta na tela tanto vertical quanto
    horizontalmente
  
  Rodando a IA, percebi que ela tem problema em perceber qual asteróide
  foi destruído se o asteróide filho tiver a mesma cor que o pai. Está
  dando uns problemas já na primeira fase

=================

17/Jul/2018
  posMethods
    findInitialObjects() -> findObjects()
    Função asteroidBFS() atualizada para que a busca dê a volta na tela,
    não considerando partes de asteróides separados pelos limites da
    tela do jogo como objetos diferentes.
  Uma descoberta sobre a forma como o emulador foi feito:
  -A linha 195 da tela do jogo é uma pela qual os asteróides passam,
  mas não ficam visíveis
  -A linha 18 da tela do jogo não mostra nenhum asteróide quando
  ele dá a volta de baixo para cima, mas aparece se der a volta de
  cima para baixo. Por causa disso, se um asteróide der a volta na
  tela ao se mover para cima, a IA enxerga as duas partes como o mesmo;
  porém, se o asteróide der a volta na tela ao se mover para baixo,
  por causa dessa linha 18 em que o asteróide não aparece nesse caso
  específico, as duas metades são consideradas asteróides diferentes.
  Esse é um caso difícil de tratar pois envolveria armazenar a velocidade
  e direção de movimento do asteróide, o que acredito que seria muito
  inconsistente e difícil de implementar.
  -Parece, na verdade, que se há, por exemplo, apenas 5 linhas de um
  asteróide na parte de cima da tela, essas 5 linhas simplesmente não
  aparecem. Ou seja, quando um asteróide está dando a volta de cima
  para baixo, chega uma hora que um pedaço de sua parte de baixo desaparece
  da mesma forma que, quando um asteróide está dando a volta de baixo para
  cima, leva um tempo até sua parte de baixo aparecer na parte de cima
  da tela

  Asteroids.py
    Função update_pos() não possui mais um comportamento diferente quando
    não encontra um asteróide na busca de sua nova posição. A nova ideia é:
    quando um asteróide é destruído (e, por consequência, uma recompensa é
    recebida), uma nova busca é feita na tela inteira de novo para ver quais
    asteróides existem e onde estão. Isso significa que o self.pos deverá
    ser atualizado

  Finalizando pelo dia: deixei a nave fazendo seus movimento pseudo-aleatórios
  até chegar na segunda fase do jogo. Não cheguei a testar se a IA estava
  sempre conseguindo acompanhar o movimento dos asteroides. Acho que é algo
  bom para se verificar amanhã. Depois disso, o próximo passo é ela enxergar
  onde a própria nave está, ângulo para onde está virada e etc.

=================

18/Jul/2018
  Classe Ship.py criada que conterá funções referentes a nave em oposição a
  classe Asteroids.py.

  Encontrada a posição inicial da nave quando o jogo começa e quando reaparece
  após perder uma vida.

=================

19/Jul/2018
  Queria tentar encontrar o significado de cada ação que a IA pode tomar, mas
  acabei fazendo o gym-retro parar de funcionar. O resto do dia foi tentando
  consertar isso

=================

20/Jul/2018
  Após não muito tempo, consegui fazer o gym-retro funcionar (reinstalando).
  Ele voltou com algumas coisas diferentes. Antes, a IA conseguia passar
  da primeira fase sem perder nenhuma vida tomando apenas ações aleatórias
  (que, pelo visto, o RNG está com uma seed com valor já atribuído, fazendo
  a IA tomar sempre as mesmas ações "aleatórias"); agora, ela perde uma ou
  duas vidas antes de passar para a segunda fase. A disposição dos asteróides
  da segunda fase parece ter mudado também.

  Outro fato interessante: antes, no arquivo pos_methods.py, a variável
  global SCREEN_LOWER_LIMIT tinha que ficar com valor 194 e a SCREEN_HEIGHT
  precisava ter o valor 177, ambos 1 a menos que nos outros arquivos. As
  contas saíam incorretas se eu usasse o mesmo valor que nos outros arquivos.
  Após conseguir reinstalar o gym-retro, eu precisei voltar esses valores para
  195 e 178 respectivamente, ficando igual aos outros arquivos (Asteroids.py
  por exemplo).

  IA agora sabe onde a nave está e inclusive consegue reencontrar se ela
  se teleportar (é um dos comandos que a nave tem). Porém, parece que essa busca
  pela nave APÓS o teleporte não está funcionando direito; parece que a IA para
  de acompanha a movimentação da nave. Preciso testar mais.

================================

21/Jul/2018
  Fiz um pequeno ajuste para que a IA voltasse a acompanhar a movimentação
  da nave após ela reaparecer, fosse o motivo do desaparecimento um teleporte
  (comando que a nave tem) ou colisão com asteroide

================================

22/Jul/2018
  A quantidade de vidas que a nave tem não diminui no exato instante em que
  a nave é destruída, mas cerca de 10 frames depois.
  Ainda sobre a quantidade de vidas, parece que a nave começa com 4 vidas.
  
  Quando um step() é dado, obs é a tela do jogo (matriz de 210, 160, 3), info
  é a quantidade de vidas e a pontuação total conseguida até o momento,
  rew é a pontuação obtida por essa ação, e done é se o jogo acabou ou não.
  Destaca-se que rew é a pontuação obtida pela ação multiplicada por 10.
  Ou seja, destruír um asteróide grande rende 20 pontos pelo que aparece
  na tela, mas o sistema entende que só 2 pontos foram recebidos. Um asteróide
  médio vale 5 pontos para o sistema e 50 para o jogador, e destruír um
  asteróide pequeno vale 10 pontos para o sistema e 100 pontos para o jogador.

  Como a colisão entre a nave e um asteróide resulta no asteróide sendo destruído,
  jogar a nave contra um asteróide é uma maneira válida de conseguir pontos. Porém,
  não é boa porque tem como custo perder uma vida. Para lidar com isso, a IA
  terá sua própria pontuação (AI_tot_reward), que terá descontos se a nave for
  destruída, diferente da pontuação do sistema (sys_tot_rew), que é apenas
  incrementado

  Preciso ainda ver outros casos para confirmar os valores, mas parece que:
  1) a nave desaparece por ~64 frames quando usa o teleporte
  2) a nave desaparece por ~128 frames quando morre, sendo que o jogo leva
     ~10 frames para atualizar a quantidade de vidas
  Ou seja, contando frames de atualização de estado e o jogo roda a 60FPS
  (que eu atraso um pouco para ser possível ver o que a nave faz), então
  o teleporte dura 1 segundo e a morte dura 2 segundos aproximadamente.
  O atributo self.blink_timer da classe Ship serve exatamente para lidar com isso.
  Quando a nave desaparece, seja por teleporte ou por morte, para evitar que 
  o programa fique lento durante esse tempo, ele espera 1 segundo para voltar a
  procurar se a nave tiver sumido por teleporte e, se tiver sido por morte,
  espera 2 segundos (já com os 10 frames de atualização da vida contados);
  O programa fica lento nesse tempo porque ele faz uma busca pela tela para ver
  se encontra a nave. enquanto esse atributo self.blink_timer for maior do que
  zero, a busca não é executada

  Ship.ast_dist é um atributo que recebe a distância da nave até cada asteróide.
  Ele é atualizado todos os frames, pois tanto a nave quanto os asteróides se movem.
  Por hora, ele armazena a distância horizontal e a distância vertical como números
  diferentes. Ou seja, NÃO calcula o tamanho da linha reta entre o centro da nave
  e o centro de cada asteróide. Ainda são necessários testes para ver se está
  acompanhando essas distâncias corretamente.
  Parece estar funcionando corretamente, mas farei mais uma leva de testes para
  garantir.

  Um corner case que eu não estou realmente com tempo de tentar arrumar para
  melhorar a fluidez que se assiste a IA aprender: caso a nave vá para o "limbo"
  da tela, ou seja, atravessar de um lado para o outro mas não aparecer porque
  existe um espaço em que isso é possível, o jogo fica lento porque passa vários
  frames buscando pela nave. Nos casos em que ela se teleporta ou morre, o número
  de frames até reaparecer é fixo, então é mais fácil lidar. Nesse, a nave poderia
  ficar até acabar a fase lá, ou já sair no frame seguinte. Como ficar sem saber
  onde a nave está é mais perigoso do que ficar buscando o tempo todo, decidi
  deixar a IA ficar procurando a nave em todos os frames que a nave estiver fora
  da tela.

================================

23/Jul/2018
  O Fábio me passou dois links falando sobre desenvolvimento de IA utilizando
  Deep Q Learning ao invés de approximate Q learning. Parece que facilitariam
  muito meu trabalho - um deles inclusive parece que servia até como uma cola
  para escrever meu código. Tenho que ter certeza de que posso utilizar esses
  links como base para fazer a minha IA, mas isso também alteraria bastante a
  proposta: talvez meu TCC se tornar um estudo sobre como a alteração nos 
  hiperparâmetros mudaria o comportamento da IA, bem como velocidade e qualidade
  de aprendizado. Um dos links é para aprendizado utilizando Tensorflow, e o
  outro utilizando PyTorch.
  https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
  https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8
  Pretendo passar este dia e talvez o próximo lendo os textos e escrevendo os
  códigos para ver mais ou menos como se comportam em seus respectivos exemplos
  (cartpole do gym no do Pytorch, e space invaders no do Tensorflow), entendê-los
  melhor e considerar sobre como eles afetariam meu TCC.

================================

24/Jul/2018
  Após receber a aprovação, comecei a fazer a IA me baseando no link que fala sobre
  desenvolver uma IA para Doom usando Tensorflow.
  O problema foi que gastei o dia inteiro só tentando instalar o TensorFlow...

================================

25/Jul/2018
  Estudei melhor o código que treina uma IA a aprender a jogar Space Invaders. O link
  para o notebook sobre isso é:
  https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Space%20Invaders/DQN%20Atari%20Space%20Invaders.ipynb

  Alguns links que podem ser úteis:
  https://www.tensorflow.org/api_docs/python/tf/keras/Sequential
  http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution

  O EP4 de aprendizado de máquina pode ser bem útil como comparativo do que e como
  fazer as coisas, apesar de ser em PyTorch ao invés de TensorFlow.

  Por hora, algumas coisas que devo fazer ficaram como comentário no código
  API do TensorFlow, mais precisamente em relação ao que deverá ser feito nas camadas ocultas:
  conv2d: https://www.tensorflow.org/api_docs/python/tf/layers/conv2d
  ELU: https://www.tensorflow.org/api_docs/python/tf/nn/elu
  max_pooling2: https://www.tensorflow.org/api_docs/python/tf/layers/max_pooling2d

================================

26/Jul/2018
  Links que explicam alguns parâmetros pedidos nas funções do Tensorflow:
  Stride: https://stackoverflow.com/questions/34642595/tensorflow-strides-argument
  Padding: https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t

  TensorBoard: https://www.youtube.com/embed/eBbEDRsCmv4

  Mais uma pequena descoberta relacionada ao Gym-Retro: 
  o guia de desenvolvimento de uma IA por Deep Q learning neural network para jogar Space
  Invaders seleciona ação de uma maneira bem específica e diferente do random_agent.py
  que veio de exemplo com o Gym-Retro.
  Existem 8 ações possíveis: UP, DOWN, LEFT, RIGHT, BUTTON, SELECT, RESET, null
  Quando uma ação é tomada, um vetor de 8 digitos é utilizado. Cada um desses 8 digitos
  pode assumir o valor de 0 ou 1 (ou seja, são tecnicamente booleanos, mas aparecem
  0 ou 1 quando impressos na tela) e corresponde a uma das 8 ações acima.
  A função utilizada pelo random_agent.py, env.action_space.sample(), preenche esse vetor
  de maneira aleatória com 0 e 1.
  Ou seja, uma ação que ele retorna pode ser, por exemplo, apertar UP, DOWN, SELECT e null.
  Essa é uma ação que não deve ser válida em nenhum jogo de Atari2600 e, como apoio a essa
  afirmação, só alguns desses botões são de fato recebidos pelo gym-retro. Em alguns testes,
  eu mandei imprimir tanto esse vetor quanto o que a ação significava. Por vezes, aparecia
  algo tipo [0 0 1 1 1 0 1 1], mas a ação era só ['DOWN', 'BUTTON'] por exemplo.
  O guia de tensorflow contorna esse problema setando apenas um desses 8 valores para 1,
  tomando apenas uma ação de cada vez. Em alguns jogos, talvez isso não seja o suficiente.
  Talvez, em alguns jogos, o agente gostaria de andar na diagonal e aperta 'UP' e 'LEFT'
  ao mesmo tempo, por exemplo, mas não é o caso do Asteroids, então vou manter esse formato.

  Preciso estudar o epsilon-greedy strategy utilizado pelo guia para entender melhor como
  esse algoritmo funciona. Além disso, o notebook fala que uma versão melhor dessa
  estratégia é utilizada no notebook de Q-learning, então é bom eu entender como essa
  versão melhor funciona.
  Além disso, o guia que estou seguindo está em um notebook, então estou um pouco confuso
  sobre o que vai e o que não vai na main(), se é que alguma coisa vai nela

  Terminei de traduzir o código do notebook para um arquivo em .py, mas preciso reler
  com mais calma para ver se o que eu fiz está certo, já que o port de um para o outro
  pareceu meio estranho para mim. Depois disso, eu testo para ver se a arquitetura
  que escolhi serve para o aprendizado da IA.

================================

27~28/Jul/2018
  O pouco que trabalhei nesses dias foi rever o código e tentar insalar TensorFlow
  que use GPU, mas não consegui fazer funcionar

================================

29/Jul/2018
  Após tentar mais um pouco instalar TensorFlow por GPU, desisti porque pareceu
  complicado demais. Vou tentar trodar uns treinamentos menores na CPU mesmo para
  ver como ele se sai. Um treinamento grande já fez o computador praticamente travar.

  Treinamentos menores também travaram. Parece que eu realmente preciso de um servidor
  para treinar minha IA, então mandei um e-mail para o meu orientador perguntando
  se eu poderia usar um do IME.

================================

05/Ago/2018
  Passei na SI e disseram que o meu orientador precisa pedir para criarem uma conta
  para mim na Rede IME para poder usá-la para treinar minha IA. Também falaram
  que talvez fosse possível na Rede Linux, mas não tem TensorFlow, então, a não
  ser que os admins instalem, não é uma opção. Nessa linha, preciso confirmar
  se tem TensorFlow na Rede IME para eu poder treinar minha IA.

  Como agora preciso esperar para ter uma conta na Rede IME para poder treinar
  minha IA, a última semana foi praticamente de folga.

  Para voltar a aproveitar meu tempo de alguma forma útil, vou estudar algumas
  monografias para ter uma ideia de como estruturar a minha. Além disso, talvez
  pesquisar alguma forma de determinar a arquitetura da IA (além de tentativa e
  erro - não creio que não haja alguma guia de como começar pelo menos).

================================

06~12/Ago/2018
  Tentativas de fazer o Gym-Retro rodar no servidor do IME e primeiros testes
  do treinamento. No início, acho que estava muito pesado, então fui diminuindo
  alguns parâmetros, como número de passos, episódios, batch_size, kernel_size,
  strides e etc. até que o servidor conseguisse rodar. Estou lidando com outros
  problemas ainda semelhantes no momento, mas acho que estou conseguindo resolvê-
  -los.
  Algumas das coisas que eu talvez possa alterar no código de treinamento:
  -retirar as partes de maxpooling2d;
  -manter ou não as linhas relacionadas ao Tensorboard

================================

13/Ago/2018
  O agente que deixam treinando desde ontem a noite não crashou!! Só que como
  desliguei o computador no meio do caminho, não sei exatamente quanto tempo
  levou. De qualquer forma, o resultado foi:

  140 pontos para
  resize: 110x84x1
  stack_size: 4
  learning_rate: 0.00025
  total_episodes: 5
  max_steps: 1000
  batch_size: 16
  explore_begin: 1.0
  explore_end: 0.01
  decay_rate: 0.0001
  gamma: 0.9
  memory_size: 10000
  conv_filter: [12, 16]
  kernel_size: [5, 3]
  stride_size: [1, 1]
  pool_kernal: [3, 2]
  SEM maxpooling

  Foi um resultado bem ruim com parâmetros bem baixos, então não foi surpreendente,
  porém, agora sei que funciona!

================================

14/Ago/2018
  Quando rodei ontem à noite o treinamento, descobri hoje de manhã que havia sido
  encerrado prematuramente porque a lista rewards_list não havia sido inicializada.
  Portanto, inicializei a lista e mandei ela ser impressa depois que o treinamento
  terminasse e voltei a rodar o programa. Era aproximadamente 9h. Por volta das
  19h, o segundo episódio de 31 havia terminado. Deve demorar um bom tempo para
  terminar, se é que vou deixar ele chegar ao fim, já que pode acabar levando
  muito mais tempo do que o tolerável. Entretanto, colocarei nesta entrada os hiper
  parâmetros utilizados nesse treinamento:
  
  resize: 110x84x1
  stack_size: 4
  learning_rate: 0.00025
  total_episodes: 31
  max_steps: 10,000
  batch_size: 32
  explore_begin: 1.0
  explore_end: 0.01
  decay_rate: 0.00001
  gamma: 0.9
  memory_size: 100,000
  conv_filters: [12, 16]
  kernel_sizes: [5, 3]
  stride_sizes: [1, 1]
  pool_kernel: [3, 2]

  Comecei a escrever a monografia e, enquanto esse treinamento segue, é bom ver
  quais parâmetros eu poderia alterar para o treinamento ser mais rápido (além de número
  máximo de passos e de episódios). Assim que os resultados desse treinamento acabarem
  (pontuação, probabilidade de exploração, training loss, etc forem imprimidos na tela),
  serão devidamente colocados aqui.
  Parece que cada episódios leva umas 4 ou 5 horas para terminar.

  Atualizei o repositório do git com o esquema da monografia em latex (créditos ao Rica),
  dei commit e push no repositório, mas lembrei que alterei um pouco o código da IA em
  outro computador, então provavelmente terei que resolver alguns conflitos do repositório
  depois....

================================

15/Ago/2018
  O modelo ainda está treinando. Após 1 dia e meio, fez 1 terço dos episódios. A pontuação
  não está muito boa até o momento.
  Para deixar anotado, as saídas até agora foram:
  Episode - Total reward - Explore prob - Training loss
  0       - 1030         - 0.9677       - 0.6024
  1       - 1180         - 0.9184       - 2.3302
  2       - 830          - 0.8955       - 0.0719
  3       - 280          - 0.8777       - 3.8982
  4       - 1030         - 0.8516       - 76.585
  5       - 730          - 0.8346       - 0.0004
  6       - 830          - 0.8167       - 12.204
  7       - 580          - 0.7989       - 0.2182
  8       - 580          - 0.7811       - 0.0008
  9       - 760          - 0.7644       - 0.0085
  10      - 930          - 0.7433       - 77.328

  Preciso dar uma olhada de por que o Training loss disparou em alguns momentos e como isso
  se relaciona com a pontuação do episódio.

  Também continuei a monografia, mais precisamente o capítulo Fundamentos. As ideias estão
  um tanto desorganizadas e o último pedaço escrito até o momento (Inteligência Artificial)
  está mais separado em tópicos por hora, para poder ser deviamente organizado mais tarde.

================================

16/Ago/2018
  Em algum momento do dia de hoje, a execução parou espontaneamente E o screen fechou. Não
  sei o que aconteceu, mas sequer posso ver os resultados dos últimos episódios que conseguiu
  rodar. O lado bom é que anotei alguns resultados ontem. Além disso, da última vez que eu vi,
  estava no episódio 14 e a pontuação não estava aumentando, então acho que não foi tão ruim.
  Agora, eu vou tentar mudar alguns parâmetros para ver se ele roda mais rápido e realmente
  aprende ao longo dos episódios.
  Um pouco triste mesmo assim.....

  Tentando rodar de novo com os parâmetros menores, o screen simplesmente estava fechado quando
  voltei. Não sei o que aconteceu, então estou tentando rodar de novo, desta vez sem usar o
  screen. Está visivelmente mais rápido (em uns 5 ou 10 minutos, já foram 4 episódios). Para
  garantir que não vou perder nada de novo, já vou começar a anotar aqui os resultados:
  Episode - Total reward - Explore prob - Training loss
  0       - 1300         - 0.96701      - 0.0619
  1       - 680          - 0.94590      - 0.0213
  2       - 510          - 0.93043      - 0.4450
  3       - 480          - 0.91459      - 0.0131
  4       - 1350         - 0.86892      - 0.1357
  5       - 380          - 0.84935      - 0.2490
  6       - 580          - 0.83183      - 0.1164
  7       - 580          - 0.81302      - 0.0177
  8       - 880          - 0.79192      - 12.235
  9       - 330          - 0.77188      - 0.2744
  10      - 1660         - 0.74121      - 0.0791
  11      - 1420         - 0.71336      - 0.0069
  12      - 680          - 0.69907      - 0.1217
  13      - 1320         - 0.67051      - 12.125
  14      - 260          - 0.66072      - 0.1081
  15      - 530          - 0.64623      - 0.0584
  16      - 460          - 0.63308      - 0.1345
  17      - 1320         - 0.60727      - 312.02
  18      - 510          - 0.59547      - 0.0720
  19      - 830          - 0.57283      - 0.1150
  20      - 830          - 0.54933      - 0.1839

  Foi até que rápido, levando em torno de 1 hora para esses 21 episódios. A pontuação que a IA
  conseguiu depois desse treinamento foid e 160! Foi bem ruim.... Mas pelo menos agora ela terminou.
  Próximo passo: alterar os hiper parâmetros até ela aprender direito!
  
  Escrevi mais um pouco da monografia também

================================

17/Ago/2018
  Rodei mais três vezes a IA com parâmetros diferentes. Seguem os parâmetros e respectivos resultados:
  Episode - Tot Rew - Explore prob - Train loss
  0       - 510     - 0.97948      - 38.4610
  1       - 530     - 0.96356      - 45.0468
  2       - 1420    - 0.88429      - 0.00262
  3       - 610     - 0.86584      - 0.32183
  4       - 980     - 0.84280      - 0.10690
  5       - 1880    - 0.77844      - 0.00006
  6       - 1300    - 0.71754      - 0.00760
  7       - 1080    - 0.69885      - 195.166
  8       - 980     - 0.67643      - 0.00475
  9       - 830     - 0.66161      - 156.220
  10      - 1540    - 0.62970      - 155.674
  11      - 780     - 0.61333      - 38.8195
  12      - 430     - 0.60476      - 0.20307
  13      - 930     - 0.58227      - 0.02497
  14      - 310     - 0.57687      - 0.16114
  15      - 330     - 0.56824      - 0.03382
  16      - 980     - 0.54118      - 0.07011
  17      - 140     - 0.53680      - 0.08106
  18      - 680     - 0.52805      - 0.01833
  19      - 980     - 0.51640      - 0.00042
  20      - 380     - 0.50898      - 0.15614

  SCORE DO MODELO APRENDIDO: 160
  
  Hiper parâmetros do teste acima:
  stack_size = 4
  learning_rate = 0.00025
  total_episodes = 21
  max_steps = 10000
  batch_size = 64
  explore_begin = 1.0
  explore_end = 0.01
  decay_rate = 0.00001
  gamma = 0.9
  memory_size = 100000
  conv_filters = [32, 64, 64]
  kernel_sizes = [8, 4, 3]
  stride_sizes = [4, 2, 2]
  pool_kernel = [3, 2] (NÃO UTILIZADO)

  ****************************************
  Episode - Tot Rew - Explore prob - Train loss
  0       - 380     - 0.98525      - 0.00637
  1       - 1470    - 0.89335      - 0.02207
  2       - 1610    - 0.84688      - 0.03757
  3       - 1130    - 0.80726      - 0.07063
  4       - 2600    - 0.75529      - 0.04968
  5       - 530     - 0.73789      - 0.07771
  6       - 780     - 0.72542      - 0.04274
  7       - 1910    - 0.68667      - 0.00684
  8       - 630     - 0.67063      - 38.6643
  9       - 430     - 0.66141      - 0.18445
  10      - 230     - 0.65426      - 0.25809
  11      - 680     - 0.64367      - 6.15982
  12      - 680     - 0.63329      - 0.01713
  13      - 930     - 0.61287      - 0.10042
  14      - 980     - 0.59549      - 0.08502
  15      - 1030    - 0.57594      - 0.03683
  16      - 1710    - 0.53498      - 0.02378
  17      - 1180    - 0.51377      - 0.10808
  18      - 1180    - 0.50402      - 0.03998
  19      - 1360    - 0.48326      - 155.283
  20      - 780     - 0.47211      - 0.20134

  SCORE DO MODELO APRENDIDO: 210

  Hiper parâmetros dos testes acima:
  stack_size = 4
  learning_rate = 0.00025
  total_episodes = 21
  max_steps = 10000
  batch_size = 64
  explore_begin = 1.0
  explore_end = 0.01
  decay_rate = 0.00001
  gamma = 0.9
  memory_size = 100000
  conv_filters = [24, 32, 32]
  kernel_sizes = [6, 3, 2]
  stride_sizes = [3, 2, 2]
  pool_kernal = [3, 2] (NÃO UTILIZADO)

  Estes foram os melhores resultados até agora, então acho que vou começar o fine tuning,
  alterando um parâmetro de cada vez para ver como melhora

  ****************************************
  Episode - Tot Rew - Explore prob - Train loss
  0       - 1300    - 0.94101      - 0.33855
  1       - 490     - 0.92465      - 0.00745
  2       - 930     - 0.90069      - 0.00012
  3       - 1300    - 0.86142      - 154.869
  4       - 680     - 0.84599      - 39.0267
  5       - 1350    - 0.81222      - 0.05511
  6       - 1180    - 0.77451      - 161.528
  7       - 630     - 0.75912      - 0.38794
  8       - 660     - 0.73645      - 0.10403
  9       - 1300    - 0.69705      - 0.17067
  10      - 880     - 0.68387      - 0.01808
  11      - 1180    - 0.65493      - 0.04504
  12      - 710     - 0.63628      - 0.00840
  13      - 980     - 0.62455      - 0.02014
  14      - 560     - 0.60952      - 0.17008
  15      - 130     - 0.59494      - 154.976
  16      - 630     - 0.58234      - 0.01038
  17      - 1030    - 0.55909      - 0.17386
  18      - 580     - 0.54652      - 0.12331
  19      - 780     - 0.52905      - 38.7622

  SCORE DO MODELO APRENDIDO: 140

  Hiper parâmetros dos testes acima:
  stack_size = 4
  learning_rate = 0.00025
  total_episodes = 21 (por algum motivo, só fez 20 episódios...)
  max_steps = 10000
  batch_size = 64
  explore_begin = 1.0
  explore_end = 0.01
  decay_rate = 0.00001
  gamma = 0.9
  memory_size = 100000
  conv_filters = [16, 32, 32]
  kernel_sizes = [6, 3, 2]
  stride_sizes = [3, 2, 2]

  Eu vou tentar aumentar o número de episódios rodados após aprender
  e guardar só o modelo que der a melhor pontuação para ver o quanto melhora.

================================

18-19/Ago/2018
  Treinei mais um pouco a IA (resultados salvos nos arquivos training_logx,
  sendo x um número, na pasta ai/). Os resultados têm sido bem ruins, então
  mandei um e-mail para o Denis perguntando se ele sabe de alguma referência
  para eu poder aprender um pouco mais sobre treinamento em deep Q learning
  e saber quais hiper parâmetros modificar e como.

  Fora isso, escrevi um pouco mais da monografia. Falta terminar de falar
  sobre IA. Como esse é o ponto mais importante do trabalho, terei que falar
  mais nessa seção, que inclui aprendizagem por reforço, deep learning e
  deep Q learning.

================================

20/Ago/2018
  Li a continuação do jupyter notebook que fala sobre como construir uma IA
  que aprende a jogar Space Invanders utilizando TensorFlow e deep Q learning.
  Essa continuação fala sobre algumas melhores no aprendizado e comecei a
  implementá-las em um outro arquivo (my_AIv2.py).
  O professor também me passou duas referências para eu aprender um pouco
  mais sobre deep learning e reinforcement learning.
