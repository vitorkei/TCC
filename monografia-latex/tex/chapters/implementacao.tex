\externaldocument{fundamentos}

%% ---------------------------------------------------------------------------- %
\chapter{Implementação}
\label{cap:implementacao}
%% ---------------------------------------------------------------------------- %

No capítulo anterior, \hyperref[cap:fundamentos]{Fundamentos}, foram apresentados os dois principais conceitos para o desenvolvimento deste trabalho: \textbf{redes neurais convolucionais} e \textbf{\textit{approximate Q-learning}}.
Para a construção da inteligência artificial que jogará o jogo \textit{Asteroids}, cada uma dessas técnicas de aprendizado possui vantagens e desvantagens.
\textbf{CNNs} são capazes de aprender características dos exemplos dados, mas eles precisam estar rotulados para isso. Uma vez que os exemplos fornecidos para o aprendizado seriam os \textit{frames} do jogo, fornecer exemplos rotulados é uma tarefa impossível, pois não há uma ação correta ou uma melhor ação na maior parte dos estados.
\textbf{\textit{Approximate Q-learning}}, por outro lado, aprende sem a necessidade de exemplos rotulados, mas precisa que as características que a IA deve aprender estejam bem definidas. Apesar de ser mais factível do que resolver o problema que a CNN enfrenta, não há garantias de que todas as características relevantes tenham sido codificadas e nem que foram bem definidas.

Portanto, a solução adotada foi unir as duas técnicas para o sucesso da IA no jogo: enquanto a parte de CNN se encarrega de identificar e aprender as características relevantes, a parte de aprendizado por reforço se preocupa em aprender quais as melhores ações para se tomar em cada momento. O resultado dessa união é uma técnica chamada de \textbf{\textit{Deep Q-Learning}} ou \textbf{\textit{Deep reinforecement learning}}.

\section{\textit{Asteroids} ver. Atari2600}
\label{sec:aa2600}

A versão de \textit{Asteroids} utilizada neste trabalho é a do Atari2600, emulada pelo emulador Stella. Nesta iteração, não existem naves espaciais inimigas, apenas asteróides que assumem três tamanhos distintos, sendo o maior deles o inicial, e três formatos diferentes, mas de aproximadamente mesma altura e largura.
Quando um asteróide grande (tamanho inicial) é destruído, outros dois de tamanho médio aparecem no lugar; após um asteróide de tamanho médio ser destruído, um de tamanho pequeno aparece em seu lugar.
Destruir um asteróide grande gera uma recompensa de 20 pontos, destruir um médio gera uma recompensa de 50, e um pequeno gera uma de 100 pontos.
A principal forma de destruir um asteróide e ganhar ponto é atirando neles, mas isso também ocorre quando há colisão entre a nave e um alvo. Isso reduz a quantidade de vidas disponíveis e, portanto, não é um método recomendado, dado que diminui a quantidade total de pontos ganha.
Os asteróides também têm uma velocidade horizontal e vertical fixa para cada um. A cada \textit{frame}, se movem 1 pixel na vertical e a cada aproximadamente 12 frames se movem um 1 pixel na horizontal, resultando em seus movimentos serem principalmente verticais.

O jogador possui cinco ações para jogar: mover-se para frente, girar a nave no sentido horário, girar a nave no sentido anti-horário, mover-se no hiperespaço, e atirar para frente.
Mover-se para frente e girar são as principais formas de movimento no jogo, enquanto atirar é a de destruir asteróides e ganhar pontos.
Mover-se no hiperespaço consiste em fazer a nave desaparecer por alguns instantes e reaparecer em um local aleatório da tela, podendo ser inclusive em cima de asteróides. Portanto, é um movimento arriscado, mas útil para fugir de situações complicadas.
O jogador tem quatro vidas inicialmente.

A tela do jogo é uma matriz de tamanho 210x160 pixels com cada pixel tendo três números, que variam de 0 a 255 cada, e que determinam sua cor de acordo com a escala RGB, tendo acesso a uma paleta de 128 cores. No topo da tela, há dois números indicando a pontuação total até o momento e quantidade de vidas restantes. Desconsiderando a moldura da tela, o espaço em que o jogo ocorre é de 177x152 pixels.

\section{Arquitetura}
\label{sec:arq}

%O treinamento da inteligência artificial deste trabalho foi separado em alguns passos distintos.

Agora que as técnicas utilizadas para o treinamento e detalhes do ambiente foram apresentados, falta descrever a arquitetura da rede e detalhes do treinamento.

Antes de um \textit{frame} ser analisado pela IA, suas cores são convertidas para escalas de cinza.
Em seguida, a moldura da tela do jogo é removida, ficando visível apenas a área de jogo por onde a nave e os asteroides transitam.
Os \textit{frames} são então redimensionados para o tamanho 150x150 pixels.

Em seguida, os \textit{frames} são inseridos em filas de tamanho quatro, com o mais antigo ficando no começo e o mais novo no final. Isso serve para que a IA tenha noção de movimento.
Por exemplo, se ela vir uma imagem estática, não saberá para onde a nave e os asteróides estão se movendo e com que velocidade.
Se vir quatro \textit{frames} em seguida, conseguirá inferir essas informações.
A cada vez que a IA vê um novo \textit{frame}, o mais antigo é descartado e o novo é inserido no final.

\begin{table}
\centering
\begin{tabular}{| p{7cm} | p{7cm} |}
  \hline
  \textbf{Hiper-parâmetro} & \textbf{Valor} \\ \hline
  Dimensões dos \textit{frames} passados para a rede & 150x150 pixels \\ \hline
  Número de \textit{frames} enfileirados & 4 \textit{frames} \\ \hline
  Primeira camada de convolução & 24 filtros de 12x12 pixels \\ \hline
  Segunda camada de convolução & 48 filtros de 6x6 pixels \\ \hline
  Número de episódios & 30 episódios \\ \hline
  Número máximo de ações por episódio & 100 000 ações \\ \hline
  Tamanho do \textit{mini-batch} & 32 \\ \hline
  Probabilidade de exploração inicial & 1.0 \\ \hline
  Probabilidade mínima de exploração & 0.1 \\ \hline
  Taxa de aprendizado $\alpha$ & 0.000025 \\ \hline
  Taxa de desconto $\gamma$ & 0.7 \\ \hline
  Tamanho da memória & 1 000 000 experiências \\ \hline
  Penalidade por morte & -500 pontos \\ \hline
\end{tabular}
\caption{Resumo da arquitetura da inteligência artificial}
\label{table:2}
\end{table}

Após esse pré processamento, os \textit{frames} são passados para a rede neural convolucional. Ela possui duas camadas de convolução, com a primeira tendo 24 filtros de tamanho 12x12 e passo de tamanho 6, e a segunda tendo 48 filtros de tamanho 6x6 e passo de tamanho 3.
Por fim, a rede possui duas camadas \textit{fully-connected}.
Em todas as camadas, é utilizada a função de ativação ELU.
Em seguida, calcula-se o erro quadrático médio das saídas para avaliar o desempenho da IA para o exemplo dado.
Como explicitado anteriormente, não é possível dizer qual a melhor ação a se tomar em cada \textit{frame} do jogo e, portanto, não seria possível calcular o erro da rede.
Para contornar esse problema, a diferença é feita entre o Q-valor desejado (parte $R(S,A,S') + \gamma\max_{A'}Q^{*}(S',A)$ da fórmula \ref{eq:qfunction}) e o Q-valor previsto para a saída da rede neural.

O treinamento teve 30 episódios, \textit{mini-batches} de tamanho 32, probabilidade de exploração inicial 1.0 que decai até um mínimo de 0.1, taxa de aprendizado $\alpha$ igual a 0.000025, e taxa de desconto $\gamma$ igual a 0.7. O agente também recebe uma penalidade de -500 pontos a cada vida perdida.

Por fim, a IA utiliza uma técnica chamada de \textit{experience replay} para melhorar o aprendizado. Ela consiste em ter uma memória que guarda experiências passadas (tuplas ($S$, $A$, $S'$ e $R$)) e utilizá-las aleatóriamente ao longo do treinamento.
Primeiro, o agente jogará o jogo uma determinada quantidade de vezes tomando apenas ações aleatórias e armazenando as experiências na memória.
Depois, quando estiver aprendendo, ao invés de passar o Q-valor desejado para o cálculo do erro da ação, a IA passa uma experiência passada.
Como o ambiente é um processo de decisão Markoviano, cada ação afeta o próximo estado. Portanto, sequências de experiências estão altamente correlacionadas. Se o agente não ajustar os pesos da rede com experiências passadas, ele passará a agir apenas de acordo com o que acabou de fazer, esquecendo o que aprendeu no passado.
A principal vantagem de utilizar \textit{experience replay} é evitar esse esquecimento.

A tabela \ref{table:2} resume a arquitetura da IA:

Por fim, a inteligência artificial utiliza o modelo construído durante o treinamento para tentar jogar sozinha. Nesta etapa, não existe aleatoriedade: se a IA jogar sempre as mesmas fases nas mesmas ordens, as ações, e por consequência, a pontuação serão sempre as mesmas.

\section{Resultados}
\label{sec:res}

Após inúmeras tentativas e erros para se chegar na arquitetura descrita \hyperref[table:2]{acima}, o melhor resultado foi de.
Jogando sem muito compromisso, consegui 3570 pontos antes de perder todas as vidas e, esforçando-me para obter uma pontuação alta, cheguei em 25130 pontos. Logo, o desempenho da IA está bem abaixo do desejado.
