de maneira geral está muito bom o texto. Acho que alguns itens estão organizados de maneira incomum, e algumas partes faltam uma descrição mais formal.

-As seções 2.1, 2.2, 2.3 e 3.1 estão descrevendo o que será feito no trabalho (e não são exatamente fundamentos teóricos). Elas ficariam melhor na introdução. Dessa forma, você poderia descrever os objetivos do trabalho com mais precisão (p. ex., implementar um jogador automático para o jogo Asteroids, usando o emulador Stella e a biblioteca tensorFlow, uma arquitetura de rede bla bla bla).

- A discussão sobre IA (primeiro parágrafo da sec. 2.4) é desnecessária e foge um pouco ao assunto (desenvolver um jogador para Asteroids). Sugiro removê-la.

- A seção sobre redes neurais 2.4.1 usa termos que sugerem um agente, quando não há o conceito de agentes nesse caso. Por exemplo, "No início, múltiplas saídas serão consideradas como boas respostas (alta probabilidade ou recompensa esperada)”; não sei se entendo o que você quer dizer aqui, mas não há alta probabilidade ou recompensa esperada aqui (não se trata de um agente em um domínio markoviano, apenas de uma função mapeando entrada e saída). Ou também: "a IA tem comportamento aleatório”; novamente, não há um agente, e a rede nunca tem um comportamento aleatório (dada que é apenas uma função). De forma geral você equaciona rede neural e IA; seria melhor apenas descrever uma rede neural como um aproximador de funções universal.

- A equação 2.3 ou 2.5 só é interessante se você explicita a dependência do erro em relação aos pesos e viéses da rede. Seria melhor simplesmente dizer que você quer otimizar os pesos para dimunuir o erro quadrático ao invés de dizer que quer calcular para onde “a rede vai se mover”… Acho que também cabe notar que minimizar o erro no conjunto de treinamento não é por si só uma coisa desejável; o que queremos é minimizar o erro sobre o conjunto de todos os dados possíveis ou pelo menos àqueles que esperamos encontrar na prática.

- Seria melhor deixar os resultados dos experimentos numa nova seção, por clareza.

- Acho que faria mais sentido ter uma sub-seção de Deep Q-Learning dentro ou depois da seção 2.4.7, explicando a ideia geral. Daí no capítulo 3 você pode explicar coisas mais específicas da sua implementação como o tipo de arquitetura escolhida (como/por que escolheu ela etc), algoritmos de otimização e inicialização de parâmetros, etc.

- A ideia de aprendizado por reforço é aproximar a política ótima em um MDP sem precisar conhecer a distribuição de probabilidade de transição, usando experiência com o ambiente. Se a distribuição é conhecida então pode-se estimar a melhor ação (ignorando problemas computacionais).

- "Uma vez que a política é gulosa, o agente sempre tomará a ação que retorna a maior recompensa imediata.” A política gulosa é gulosa em relação à função Q; ela não escolhe a ação com maior recompensa imediata (isso seria maximizar R(s,a)). Acho que a questão que você colocar aqui é a diferença entre planejamento a longo ou curto prazo (sendo otimizar a recompensa imediata curtíssimo prazo). 

- Faltou falar (e formalizar) epsilon-greedy policy quando você menciona Exploração  vs. Eploitação.

- Approximate Q-Learning é qualquer método que aproxime a função Q por uma classe de funções, por exemplo funções lineares… dessa forma Deep Q -Learning é um tipo de approximate Q-Learning.

- Seria legal ter um exemplo de tela do jogo na introdução, talvez anotado com asteróides, nave, etc.

Um comentário ortogonal ao texto: você diz que tentou outras arquiteturas e acabou usando a Double Deep Q-Learning com Dueling (que me parece bem sofisticada); você tentou fazer algo mais simples (uma única rede com entrada S e saídas Q(S,a) para cada ação a)?

- Acho que antes de começar a descrever a rede usada, é importante formalizar o MDP que modela o problema: dizer o que é S (conjunto de imagens 150x150 representando a intensidade dos pixels depois de bla bla bla), A (ações), R(S,A) e (...)

- A eq 3.1 é um pouco confusa: se Q(S,A) no lado esquerdo e Q(S’,argmax …) no lado direito são redes distintas, então seria melhor usar nomes diferentes para as funções (p.ex., Q_1(S,A) = … + Q_2(S’,…)). De qualquer forma acho que a equação tem algo faltando, pois esses termos deveriam entrar na função de custo/erro das redes (não sei bem como nesse caso).

- (...) P(S,A,S'), essa última apenas dizendo o que seriam essas probabilidades (que serão implicitamente estimadas pelo algoritmo).
